# LLM Модуль для Ollama

Этот модуль предоставляет простой и изолированный сервис для взаимодействия с локально запущенным сервером Ollama. Он предназначен для интеграции в TypeScript проекты.

## Функционал

- **Генерация текста**: Отправка запросов к LLM (по умолчанию `gemma3:27b`) и получение ответа.
- **Проверка доступности**: Функции для проверки, запущен ли сервер Ollama и доступна ли на нем целевая модель.

---

## 1. Настройка

Перед использованием этого модуля убедитесь, что вы выполнили следующие шаги:

### а) Установка и запуск Ollama

Сервер Ollama должен быть установлен и запущен на вашей машине.
Подробнее: [https://ollama.com/](https://ollama.com/)

### б) Загрузка модели

Вам необходимо скачать модель, с которой будет работать сервис. Выполните в терминале:

```bash
ollama pull gemma3:27b
```

### в) Настройка CORS (Cross-Origin Resource Sharing)

При использовании этого модуля из веб-приложения (например, React), которое работает на `localhost:3000`, вы столкнетесь с ошибками CORS, так как по умолчанию Ollama API не принимает запросы с других "источников" (origins).

Чтобы это исправить, необходимо задать переменную окружения `OLLAMA_ORIGINS` перед запуском сервера Ollama.

**Пример для macOS/Linux:**

```bash
OLLAMA_ORIGINS=http://localhost:3000 ollama serve
```
*Замените `http://localhost:3000` на адрес вашего dev-сервера, если он отличается.*

---

## 2. Использование

Ниже приведен пример, как можно интегрировать сервис в ваш код.

```typescript
import { 
  isOllamaRunning, 
  checkOllamaModel, 
  generateText 
} from './llm/ollamaService'; // Укажите правильный путь к файлу

async function initializeAndAskAI() {
  console.log('Проверяем статус Ollama...');
  
  if (!await isOllamaRunning()) {
    console.error('Сервис Ollama не запущен. Запустите его и перезагрузите страницу.');
    // Здесь можно показать ошибку в UI
    return;
  }
  
  console.log('Сервер найден. Проверяем наличие модели "gemma3:27b"...');
  
  if (!await checkOllamaModel()) {
    console.error('Модель "gemma3:27b" не найдена. Установите ее командой: ollama pull gemma3:27b');
    // Показать ошибку в UI
    return;
  }

  console.log('Модель найдена. Отправляем запрос...');

  try {
    const prompt = "Расскажи короткую шутку про программиста";
    const aiResponse = await generateText(prompt);
    
    console.log('Ответ от AI:', aiResponse);
    // Отобразить ответ в UI
    
  } catch (error) {
    console.error('Ошибка при генерации текста:', error);
    // Показать ошибку в UI
  }
}

// Вызовите эту функцию при загрузке компонента или по клику на кнопку
initializeAndAskAI();
```
